{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41feac8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be8fcc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load and clean data\n",
    "def load_and_prepare_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y')\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    # Handle any missing values\n",
    "    df['Price'] = df['Price'].interpolate(method='linear')\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df['Log_Return'] = np.log(df['Price']) - np.log(df['Price'].shift(1))\n",
    "    df['Return'] = df['Price'].pct_change()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Visualize time series\n",
    "def plot_price_series(df):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Raw price series\n",
    "    axes[0].plot(df['Date'], df['Price'], color='steelblue', linewidth=1)\n",
    "    axes[0].set_title('Brent Crude Oil Prices (1987-2022)', fontsize=14)\n",
    "    axes[0].set_ylabel('Price (USD/barrel)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log returns\n",
    "    axes[1].plot(df['Date'], df['Log_Return'], color='coral', linewidth=0.5)\n",
    "    axes[1].set_title('Daily Log Returns', fontsize=14)\n",
    "    axes[1].set_ylabel('Log Return')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volatility (rolling standard deviation)\n",
    "    df['Volatility'] = df['Log_Return'].rolling(window=30).std() * np.sqrt(252)\n",
    "    axes[2].plot(df['Date'], df['Volatility'], color='purple', linewidth=1)\n",
    "    axes[2].set_title('30-Day Rolling Volatility', fontsize=14)\n",
    "    axes[2].set_ylabel('Annualized Volatility')\n",
    "    axes[2].set_xlabel('Date')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('price_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315dac51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_change_point_model(data, n_changepoints=5):\n",
    "    \"\"\"\n",
    "    Bayesian multiple change point model for oil prices\n",
    "    \"\"\"\n",
    "    with pm.Model() as model:\n",
    "        # Data\n",
    "        prices = data['Price'].values\n",
    "        n_obs = len(prices)\n",
    "        \n",
    "        # Priors for change points\n",
    "        tau = pm.DiscreteUniform('tau', \n",
    "                                 lower=0, \n",
    "                                 upper=n_obs-1, \n",
    "                                 shape=n_changepoints)\n",
    "        \n",
    "        # Sort change points\n",
    "        tau_sorted = pm.Deterministic('tau_sorted', \n",
    "                                      pm.math.sort(tau))\n",
    "        \n",
    "        # Segment means and standard deviations\n",
    "        segment_means = []\n",
    "        segment_sigmas = []\n",
    "        \n",
    "        for i in range(n_changepoints + 1):\n",
    "            if i == 0:\n",
    "                start_idx = 0\n",
    "            else:\n",
    "                start_idx = tau_sorted[i-1]\n",
    "            \n",
    "            if i == n_changepoints:\n",
    "                end_idx = n_obs\n",
    "            else:\n",
    "                end_idx = tau_sorted[i]\n",
    "            \n",
    "            # Priors for segment parameters\n",
    "            mu = pm.Normal(f'mu_{i}', mu=np.mean(prices), sigma=np.std(prices)*2)\n",
    "            sigma = pm.HalfNormal(f'sigma_{i}', sigma=np.std(prices))\n",
    "            \n",
    "            segment_means.append(mu)\n",
    "            segment_sigmas.append(sigma)\n",
    "        \n",
    "        # Likelihood using switch function\n",
    "        idx = np.arange(n_obs)\n",
    "        mu_combined = segment_means[0]\n",
    "        sigma_combined = segment_sigmas[0]\n",
    "        \n",
    "        for i in range(1, n_changepoints + 1):\n",
    "            mask = idx >= tau_sorted[i-1]\n",
    "            mu_combined = pm.math.switch(mask, segment_means[i], mu_combined)\n",
    "            sigma_combined = pm.math.switch(mask, segment_sigmas[i], sigma_combined)\n",
    "        \n",
    "        # Likelihood\n",
    "        likelihood = pm.Normal('likelihood', \n",
    "                               mu=mu_combined, \n",
    "                               sigma=sigma_combined, \n",
    "                               observed=prices)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_mcmc(model, draws=3000, tune=1000):\n",
    "    \"\"\"\n",
    "    Run MCMC sampling\n",
    "    \"\"\"\n",
    "    with model:\n",
    "        # Sampling\n",
    "        trace = pm.sample(draws=draws, \n",
    "                         tune=tune, \n",
    "                         chains=4, \n",
    "                         cores=4,\n",
    "                         return_inferencedata=True,\n",
    "                         progressbar=True)\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53ef0ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_results(trace, data, events_df):\n",
    "    \"\"\"\n",
    "    Analyze and visualize MCMC results\n",
    "    \"\"\"\n",
    "    # Convergence diagnostics\n",
    "    summary = az.summary(trace, var_names=['tau', 'mu_', 'sigma_'])\n",
    "    print(\"Model Summary:\")\n",
    "    print(summary)\n",
    "    \n",
    "    # Trace plots\n",
    "    az.plot_trace(trace, var_names=['tau'])\n",
    "    plt.suptitle('MCMC Trace Plots for Change Points', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('trace_plots.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Posterior distributions of change points\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, tau_var in enumerate(['tau_sorted[0]', 'tau_sorted[1]', 'tau_sorted[2]', \n",
    "                                 'tau_sorted[3]', 'tau_sorted[4]']):\n",
    "        if tau_var in trace.posterior:\n",
    "            tau_samples = trace.posterior[tau_var].values.flatten()\n",
    "            \n",
    "            # Convert to dates\n",
    "            tau_dates = data['Date'].iloc[tau_samples.astype(int)].values\n",
    "            \n",
    "            axes[i].hist(tau_dates, bins=50, alpha=0.7, color='steelblue')\n",
    "            axes[i].set_title(f'Change Point {i+1}')\n",
    "            axes[i].set_xlabel('Date')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            \n",
    "            # Add event markers\n",
    "            for _, event in events_df.iterrows():\n",
    "                event_date = pd.to_datetime(event['Date'])\n",
    "                axes[i].axvline(x=event_date, color='red', alpha=0.5, linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('change_point_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def quantify_impacts(trace, data, change_point_idx):\n",
    "    \"\"\"\n",
    "    Quantify price impacts before and after change points\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, cp_idx in enumerate(change_point_idx):\n",
    "        # Get samples for segment parameters\n",
    "        mu_before = trace.posterior[f'mu_{i}'].values.flatten()\n",
    "        mu_after = trace.posterior[f'mu_{i+1}'].values.flatten()\n",
    "        \n",
    "        # Calculate impact metrics\n",
    "        mean_before = np.mean(mu_before)\n",
    "        mean_after = np.mean(mu_after)\n",
    "        percent_change = ((mean_after - mean_before) / mean_before) * 100\n",
    "        \n",
    "        # Credible intervals\n",
    "        ci_before = np.percentile(mu_before, [2.5, 97.5])\n",
    "        ci_after = np.percentile(mu_after, [2.5, 97.5])\n",
    "        \n",
    "        # Find closest event\n",
    "        cp_date = data['Date'].iloc[cp_idx]\n",
    "        events_df['Date_dt'] = pd.to_datetime(events_df['Date'])\n",
    "        closest_event = events_df.iloc[(events_df['Date_dt'] - cp_date).abs().argsort()[:1]]\n",
    "        \n",
    "        results.append({\n",
    "            'change_point': i+1,\n",
    "            'date': cp_date,\n",
    "            'mean_before': mean_before,\n",
    "            'mean_after': mean_after,\n",
    "            'percent_change': percent_change,\n",
    "            'ci_before': ci_before,\n",
    "            'ci_after': ci_after,\n",
    "            'closest_event': closest_event['Event Name'].values[0] if not closest_event.empty else 'Unknown',\n",
    "            'event_date': closest_event['Date'].values[0] if not closest_event.empty else None\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f62d0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def correlate_events_with_changepoints(change_points_df, events_df, window_days=30):\n",
    "    \"\"\"\n",
    "    Correlate detected change points with historical events\n",
    "    \"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    for _, cp in change_points_df.iterrows():\n",
    "        cp_date = cp['date']\n",
    "        \n",
    "        # Find events within window\n",
    "        events_df['Date_dt'] = pd.to_datetime(events_df['Date'])\n",
    "        time_diffs = (events_df['Date_dt'] - cp_date).abs()\n",
    "        nearby_events = events_df[time_diffs.dt.days <= window_days]\n",
    "        \n",
    "        for _, event in nearby_events.iterrows():\n",
    "            # Calculate pre/post event statistics\n",
    "            event_date = event['Date_dt']\n",
    "            pre_window = data[(data['Date'] >= event_date - pd.Timedelta(days=window_days)) & \n",
    "                             (data['Date'] < event_date)]\n",
    "            post_window = data[(data['Date'] > event_date) & \n",
    "                              (data['Date'] <= event_date + pd.Timedelta(days=window_days))]\n",
    "            \n",
    "            if len(pre_window) > 0 and len(post_window) > 0:\n",
    "                pre_mean = pre_window['Price'].mean()\n",
    "                post_mean = post_window['Price'].mean()\n",
    "                price_change = ((post_mean - pre_mean) / pre_mean) * 100\n",
    "                \n",
    "                correlations.append({\n",
    "                    'change_point_date': cp_date,\n",
    "                    'event_name': event['Event Name'],\n",
    "                    'event_date': event_date,\n",
    "                    'days_difference': (event_date - cp_date).days,\n",
    "                    'pre_event_mean': pre_mean,\n",
    "                    'post_event_mean': post_mean,\n",
    "                    'price_change_percent': price_change,\n",
    "                    'event_category': event['Category']\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd71838",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    data = load_and_prepare_data('data/raw/brent_prices.csv')\n",
    "    events_df = pd.read_csv('data/processed/events_dataset.csv')\n",
    "    \n",
    "    # EDA\n",
    "    print(\"Performing exploratory analysis...\")\n",
    "    plot_price_series(data)\n",
    "    \n",
    "    # Build and run model\n",
    "    print(\"Building Bayesian change point model...\")\n",
    "    model = bayesian_change_point_model(data, n_changepoints=5)\n",
    "    \n",
    "    print(\"Running MCMC sampling...\")\n",
    "    trace = run_mcmc(model)\n",
    "    \n",
    "    # Analyze results\n",
    "    print(\"Analyzing results...\")\n",
    "    summary = analyze_results(trace, data, events_df)\n",
    "    \n",
    "    # Get most probable change points\n",
    "    tau_samples = trace.posterior['tau_sorted'].values\n",
    "    most_probable_cps = np.median(tau_samples, axis=(0,1)).astype(int)\n",
    "    \n",
    "    # Quantify impacts\n",
    "    print(\"Quantifying impacts...\")\n",
    "    impacts_df = quantify_impacts(trace, data, most_probable_cps)\n",
    "    print(\"\\nImpact Analysis:\")\n",
    "    print(impacts_df.to_string())\n",
    "    \n",
    "    # Correlate with events\n",
    "    correlations_df = correlate_events_with_changepoints(\n",
    "        impacts_df, events_df, window_days=45\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvent Correlations:\")\n",
    "    print(correlations_df.sort_values('price_change_percent', ascending=False).head(10))\n",
    "    \n",
    "    # Save results\n",
    "    impacts_df.to_csv('results/change_point_impacts.csv', index=False)\n",
    "    correlations_df.to_csv('results/event_correlations.csv', index=False)\n",
    "    \n",
    "    return data, trace, impacts_df, correlations_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data, trace, impacts, correlations = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47a6652",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
