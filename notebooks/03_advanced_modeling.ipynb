{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337cc90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Advanced Modeling for Brent Oil Prices\n",
    "\n",
    "## 1. Setup and Imports\n",
    "\n",
    "```python\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from src.utils.data_loader import load_brent_prices, load_events_data\n",
    "from src.utils.visualization import plot_regime_analysis, plot_event_impact\n",
    "from src.models.advanced_models import VectorAutoregressionModel, MarkovSwitchingModel, GaussianMixtureRegimeDetection\n",
    "\n",
    "# Import statsmodels\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, coint\n",
    "from statsmodels.tsa.api import VAR, VARMAX\n",
    "from statsmodels.tsa.vector_ar.var_model import VARResults\n",
    "from arch import arch_model\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10413777",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "df = pd.read_csv('../data/processed/cleaned_prices.csv', parse_dates=['Date'])\n",
    "events_df = pd.read_csv('../data/processed/events_dataset.csv', parse_dates=['Date'])\n",
    "\n",
    "print(f\"Data loaded: {len(df)} observations\")\n",
    "print(f\"Events loaded: {len(events_df)} events\")\n",
    "\n",
    "# Display recent data\n",
    "print(\"\\nRecent Data:\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e19ca9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create synthetic macroeconomic data for demonstration\n",
    "# In practice, you would use actual macroeconomic data\n",
    "np.random.seed(42)\n",
    "n = len(df)\n",
    "\n",
    "# Create synthetic macroeconomic variables\n",
    "macro_vars = pd.DataFrame({\n",
    "    'Date': df['Date'],\n",
    "    'GDP_Growth': np.random.normal(2.5, 1.5, n) + 0.01 * np.sin(2 * np.pi * np.arange(n) / 252),\n",
    "    'Inflation': np.random.normal(2.0, 1.0, n) + 0.0005 * np.arange(n),\n",
    "    'USD_Index': np.random.normal(90, 10, n),\n",
    "    'VIX': np.random.normal(20, 5, n) + df['Return'].abs() * 100,\n",
    "})\n",
    "\n",
    "# Merge with price data\n",
    "var_data = pd.merge(df[['Date', 'Price', 'Return', 'Volatility_30']], \n",
    "                    macro_vars, on='Date', how='inner')\n",
    "var_data = var_data.set_index('Date')\n",
    "\n",
    "print(\"VAR Data Shape:\", var_data.shape)\n",
    "print(\"\\nVariables:\")\n",
    "print(var_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9930640",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def perform_granger_causality(data, max_lag=10):\n",
    "    \"\"\"\n",
    "    Perform Granger causality tests between variables\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    variables = data.columns.tolist()\n",
    "    \n",
    "    for cause in variables:\n",
    "        for effect in variables:\n",
    "            if cause != effect:\n",
    "                test_data = data[[effect, cause]].dropna()\n",
    "                \n",
    "                if len(test_data) > max_lag * 2:\n",
    "                    try:\n",
    "                        granger_test = grangercausalitytests(test_data, maxlag=max_lag, verbose=False)\n",
    "                        \n",
    "                        # Get p-values for different lags\n",
    "                        p_values = [granger_test[i+1][0]['ssr_ftest'][1] for i in range(max_lag)]\n",
    "                        min_p_value = min(p_values)\n",
    "                        best_lag = p_values.index(min_p_value) + 1\n",
    "                        \n",
    "                        if min_p_value < 0.05:\n",
    "                            results[(cause, effect)] = {\n",
    "                                'best_lag': best_lag,\n",
    "                                'p_value': min_p_value,\n",
    "                                'causal': True\n",
    "                            }\n",
    "                    except:\n",
    "                        continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform Granger causality tests\n",
    "print(\"Performing Granger causality tests...\")\n",
    "granger_results = perform_granger_causality(var_data[['Price', 'VIX', 'USD_Index']].dropna(), max_lag=5)\n",
    "\n",
    "print(\"\\nSignificant Granger Causality Results (p < 0.05):\")\n",
    "for (cause, effect), stats in granger_results.items():\n",
    "    if stats['causal']:\n",
    "        print(f\"{cause} -> {effect}: lag={stats['best_lag']}, p={stats['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b996868",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test_cointegration(series1, series2, series1_name, series2_name):\n",
    "    \"\"\"\n",
    "    Test for cointegration between two series\n",
    "    \"\"\"\n",
    "    # Drop NaN values and align indices\n",
    "    aligned_data = pd.concat([series1, series2], axis=1).dropna()\n",
    "    if len(aligned_data) < 10:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        coint_result = coint(aligned_data.iloc[:, 0], aligned_data.iloc[:, 1])\n",
    "        \n",
    "        return {\n",
    "            'series1': series1_name,\n",
    "            'series2': series2_name,\n",
    "            'test_statistic': coint_result[0],\n",
    "            'p_value': coint_result[1],\n",
    "            'critical_values': coint_result[2],\n",
    "            'is_cointegrated': coint_result[1] < 0.05\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Test cointegration between key variables\n",
    "print(\"Cointegration Tests:\")\n",
    "coint_tests = [\n",
    "    ('Price', 'GDP_Growth'),\n",
    "    ('Price', 'USD_Index'),\n",
    "    ('Price', 'VIX'),\n",
    "]\n",
    "\n",
    "for var1, var2 in coint_tests:\n",
    "    result = test_cointegration(var_data[var1], var_data[var2], var1, var2)\n",
    "    if result:\n",
    "        status = \"COINTEGRATED\" if result['is_cointegrated'] else \"Not cointegrated\"\n",
    "        print(f\"{var1} vs {var2}: {status} (p={result['p_value']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734cac7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare data for VAR\n",
    "var_model_data = var_data[['Price', 'VIX', 'USD_Index']].dropna()\n",
    "\n",
    "# Create and fit VAR model\n",
    "var_model = VectorAutoregressionModel(max_lags=10)\n",
    "var_results = var_model.fit(var_model_data, lags=2)\n",
    "\n",
    "# Generate forecasts\n",
    "forecast_steps = 30\n",
    "forecasts = var_model.forecast(steps=forecast_steps)\n",
    "\n",
    "print(f\"\\nVAR Forecasts (next {forecast_steps} days):\")\n",
    "print(forecasts.head())\n",
    "\n",
    "# Plot forecasts\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "for idx, var in enumerate(['Price', 'VIX', 'USD_Index']):\n",
    "    # Historical data\n",
    "    axes[idx].plot(var_model_data.index[-100:], var_model_data[var][-100:], \n",
    "                   label='Historical', color='steelblue', linewidth=2)\n",
    "    \n",
    "    # Forecast\n",
    "    forecast_dates = pd.date_range(start=var_model_data.index[-1], \n",
    "                                   periods=forecast_steps+1, freq='D')[1:]\n",
    "    axes[idx].plot(forecast_dates, forecasts[var], \n",
    "                   label='Forecast', color='red', linewidth=2, linestyle='--')\n",
    "    \n",
    "    axes[idx].set_title(f'{var} - VAR Forecast', fontsize=12)\n",
    "    axes[idx].set_ylabel(var)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/var_forecasts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaaaedd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fit Markov Switching model\n",
    "print(\"Fitting Markov Switching Model...\")\n",
    "ms_model = MarkovSwitchingModel(n_regimes=3)\n",
    "ms_results = ms_model.fit(df['Price'].dropna(), model_type='mean_var')\n",
    "\n",
    "# Get regime probabilities\n",
    "regime_probs = ms_model.get_regime_probabilities()\n",
    "\n",
    "# Plot regime probabilities\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot price\n",
    "axes[0].plot(df['Date'], df['Price'], color='steelblue', linewidth=1)\n",
    "axes[0].set_ylabel('Price (USD)')\n",
    "axes[0].set_title('Brent Price with Regime Probabilities')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot regime probabilities\n",
    "for i in range(3):\n",
    "    axes[1].fill_between(df['Date'][:len(regime_probs)], \n",
    "                        regime_probs.iloc[:, i], \n",
    "                        alpha=0.5, label=f'Regime {i+1}')\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot most likely regime\n",
    "most_likely_regime = regime_probs.idxmax(axis=1)\n",
    "axes[2].plot(df['Date'][:len(most_likely_regime)], most_likely_regime, \n",
    "            color='green', linewidth=2, drawstyle='steps-post')\n",
    "axes[2].set_ylabel('Regime')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_yticks([0, 1, 2])\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/markov_regimes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print regime statistics\n",
    "print(\"\\nRegime Statistics:\")\n",
    "params = ms_model.get_regime_parameters()\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4a10f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Use GMM for regime detection\n",
    "print(\"\\nGaussian Mixture Model for Regime Detection...\")\n",
    "gmm_model = GaussianMixtureRegimeDetection(n_regimes=3)\n",
    "\n",
    "# Extract features\n",
    "features = gmm_model.extract_features(df['Price'], window=30)\n",
    "print(f\"Extracted {len(features)} features\")\n",
    "\n",
    "# Fit GMM\n",
    "features_with_regimes = gmm_model.fit(features)\n",
    "\n",
    "# Plot regimes\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot price colored by regime\n",
    "scatter = axes[0].scatter(df['Date'][:len(features_with_regimes)], \n",
    "                         df['Price'][:len(features_with_regimes)], \n",
    "                         c=features_with_regimes['regime'], \n",
    "                         cmap='viridis', s=10, alpha=0.6)\n",
    "axes[0].set_ylabel('Price (USD)')\n",
    "axes[0].set_title('Price Colored by GMM Regime')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Regime')\n",
    "\n",
    "# Plot regime statistics\n",
    "regime_stats = pd.DataFrame(gmm_model.regime_stats).T\n",
    "axes[1].bar(regime_stats.index, regime_stats['probability'], \n",
    "           color=['red', 'green', 'blue'], alpha=0.6)\n",
    "axes[1].set_xlabel('Regime')\n",
    "axes[1].set_ylabel('Probability')\n",
    "axes[1].set_title('Regime Probabilities')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/gmm_regimes.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGMM Regime Statistics:\")\n",
    "print(regime_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86482bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze impact of major events\n",
    "print(\"\\nEvent Study Analysis\")\n",
    "\n",
    "# Select major events\n",
    "major_events = events_df[events_df['Impact_Score'] >= 2]\n",
    "\n",
    "event_impacts = []\n",
    "\n",
    "for _, event in major_events.iterrows():\n",
    "    event_date = event['Date']\n",
    "    event_name = event['Event Name']\n",
    "    \n",
    "    # Calculate pre- and post-event returns\n",
    "    pre_window = 30\n",
    "    post_window = 30\n",
    "    \n",
    "    # Find event in data\n",
    "    event_idx = (df['Date'] - event_date).abs().argmin()\n",
    "    actual_event_date = df.iloc[event_idx]['Date']\n",
    "    \n",
    "    if event_idx > pre_window and event_idx < len(df) - post_window:\n",
    "        # Pre-event returns\n",
    "        pre_returns = df['Return'].iloc[event_idx-pre_window:event_idx].values\n",
    "        pre_mean_return = np.mean(pre_returns) * 100\n",
    "        \n",
    "        # Post-event returns\n",
    "        post_returns = df['Return'].iloc[event_idx:event_idx+post_window].values\n",
    "        post_mean_return = np.mean(post_returns) * 100\n",
    "        \n",
    "        # Calculate impact\n",
    "        impact = post_mean_return - pre_mean_return\n",
    "        \n",
    "        event_impacts.append({\n",
    "            'Event': event_name,\n",
    "            'Date': actual_event_date.date(),\n",
    "            'Category': event['Category'],\n",
    "            'Pre_Event_Return': pre_mean_return,\n",
    "            'Post_Event_Return': post_mean_return,\n",
    "            'Impact': impact,\n",
    "            'Absolute_Impact': abs(impact)\n",
    "        })\n",
    "\n",
    "# Create impact DataFrame\n",
    "impact_df = pd.DataFrame(event_impacts).sort_values('Absolute_Impact', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Events by Absolute Impact:\")\n",
    "print(impact_df[['Event', 'Date', 'Category', 'Impact']].head(10).to_string())\n",
    "\n",
    "# Plot event impacts\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# Sort by impact\n",
    "impact_df_sorted = impact_df.sort_values('Impact')\n",
    "\n",
    "# Create bar plot\n",
    "colors = ['red' if x < 0 else 'green' for x in impact_df_sorted['Impact']]\n",
    "bars = ax.barh(range(len(impact_df_sorted)), impact_df_sorted['Impact'], color=colors, alpha=0.6)\n",
    "ax.set_yticks(range(len(impact_df_sorted)))\n",
    "ax.set_yticklabels(impact_df_sorted['Event'])\n",
    "ax.set_xlabel('Impact on Returns (%)')\n",
    "ax.set_title('Event Impact Analysis (30-day window)')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width if width >= 0 else width - 0.5, bar.get_y() + bar.get_height()/2,\n",
    "            f'{width:.1f}%', ha='left' if width >= 0 else 'right', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/event_impacts.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ac2e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Compare different models\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate metrics for comparison\n",
    "models_metrics = {}\n",
    "\n",
    "# 1. Bayesian Change Point Model (from Task 2)\n",
    "print(\"\\n1. Bayesian Change Point Model:\")\n",
    "print(\"   • Strengths: Identifies structural breaks, provides uncertainty estimates\")\n",
    "print(\"   • Weaknesses: Computationally intensive, assumes abrupt changes\")\n",
    "print(\"   • Best for: Detecting regime shifts, event impact analysis\")\n",
    "\n",
    "# 2. VAR Model\n",
    "print(\"\\n2. Vector Autoregression (VAR) Model:\")\n",
    "print(\"   • Strengths: Captures interdependencies, good for forecasting\")\n",
    "print(\"   • Weaknesses: Requires stationary data, sensitive to lag selection\")\n",
    "print(\"   • Best for: Multi-variable analysis, short-term forecasting\")\n",
    "\n",
    "# 3. Markov Switching Model\n",
    "print(\"\\n3. Markov Switching Model:\")\n",
    "print(\"   • Strengths: Explicit regime modeling, handles smooth transitions\")\n",
    "print(\"   • Weaknesses: Complex estimation, may overfit\")\n",
    "print(\"   • Best for: Regime detection, risk management\")\n",
    "\n",
    "# 4. GARCH Model\n",
    "print(\"\\n4. GARCH Model:\")\n",
    "print(\"   • Strengths: Captures volatility clustering, good for risk metrics\")\n",
    "print(\"   • Weaknesses: Assumes symmetric response, may miss structural breaks\")\n",
    "print(\"   • Best for: Volatility forecasting, risk measurement\")\n",
    "\n",
    "# 5. Gaussian Mixture Model\n",
    "print(\"\\n5. Gaussian Mixture Model:\")\n",
    "print(\"   • Strengths: Flexible regime detection, handles complex distributions\")\n",
    "print(\"   • Weaknesses: No temporal structure, may be sensitive to initialization\")\n",
    "print(\"   • Best for: Market regime classification, feature-based analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS FOR STAKEHOLDERS:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFor Investors:\")\n",
    "print(\"   • Use Bayesian change point for risk management triggers\")\n",
    "print(\"   • Employ Markov switching for portfolio regime adjustments\")\n",
    "print(\"   • Monitor GARCH volatility for position sizing\")\n",
    "\n",
    "print(\"\\nFor Policymakers:\")\n",
    "print(\"   • VAR models for understanding market interdependencies\")\n",
    "print(\"   • Event study analysis for policy impact assessment\")\n",
    "print(\"   • Regime detection for early warning systems\")\n",
    "\n",
    "print(\"\\nFor Energy Companies:\")\n",
    "print(\"   • Combine multiple models for robust forecasting\")\n",
    "print(\"   • Use change point analysis for contract timing\")\n",
    "print(\"   • Implement regime-aware hedging strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8633920",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FUTURE WORK AND EXTENSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Machine Learning Approaches:\")\n",
    "print(\"   • LSTM/GRU neural networks for price forecasting\")\n",
    "print(\"   • Random Forests for feature importance analysis\")\n",
    "print(\"   • XGBoost for regime classification\")\n",
    "\n",
    "print(\"\\n2. Advanced Bayesian Methods:\")\n",
    "print(\"   • Bayesian Structural Time Series (BSTS)\")\n",
    "print(\"   • Gaussian Processes for uncertainty quantification\")\n",
    "print(\"   • Hierarchical models for multiple time series\")\n",
    "\n",
    "print(\"\\n3. Alternative Data Integration:\")\n",
    "print(\"   • Satellite imagery for inventory tracking\")\n",
    "print(\"   • News sentiment analysis from financial media\")\n",
    "print(\"   • Shipping and logistics data for supply chain insights\")\n",
    "\n",
    "print(\"\\n4. Real-time Analysis:\")\n",
    "print(\"   • Streaming data pipelines for live monitoring\")\n",
    "print(\"   • Automated event detection from news feeds\")\n",
    "print(\"   • Real-time change point alerts\")\n",
    "\n",
    "print(\"\\n5. Causal Inference:\")\n",
    "print(\"   • Difference-in-differences for policy evaluation\")\n",
    "print(\"   • Synthetic control methods for counterfactual analysis\")\n",
    "print(\"   • Instrumental variable approaches for causal effects\")\n",
    "\n",
    "# Save results\n",
    "impact_df.to_csv('../results/event_impact_analysis.csv', index=False)\n",
    "print(\"\\nResults saved to ../results/event_impact_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35008b98",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
